{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b81687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-4f8c8309-37ce-4677-88bf-3a4c74293690.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpu', type=int, default=4, help='Interval to be displayed')\n",
    "# parser.add_argument('--gpu', type=int, default=2000, help='Interval to be displayed')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# parser.add_argument('--gpu', type=int, default=4, help='')\n",
    "# parser.add_argument('--world_size', type=int, default=4, help='')\n",
    "# parser.add_argument('--rank', type=int, default=4, help='')\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    args.world_size = ngpus_per_node * args.world_size\n",
    "    args.rank = 0\n",
    "    \n",
    "    print(\"메인\")\n",
    "    mp.spawn(main_worker, nprocs=ngpus_per_node, \n",
    "             args=(ngpus_per_node, args))\n",
    "    \n",
    "    \n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    global best_acc1\n",
    "    args.gpu = gpu\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "    print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "    args.rank = args.rank * ngpus_per_node + gpu\n",
    "    dist.init_process_group(backend='nccl', \n",
    "                            init_method='tcp://127.0.0.1:80',\n",
    "                            world_size=args.world_size, \n",
    "                            rank=args.rank)\n",
    "    \n",
    "#     model = Bert()\n",
    "#     model.cuda(args.gpu)\n",
    "#     model = DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "\n",
    "    acc = 0\n",
    "    print(\"잘됨\")\n",
    "#     for i in range(args.num_epochs):\n",
    "#         model = train(model)\n",
    "#         acc = test(model, acc)\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b239f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel import DataParallelCriterion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
